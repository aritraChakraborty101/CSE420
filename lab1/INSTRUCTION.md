# Lexical and Syntax Analyzer - Complete Guide

## Table of Contents
1. [Project Overview](#project-overview)
2. [Compilation Process](#compilation-process)
3. [Component Breakdown](#component-breakdown)
4. [How Everything Works Together](#how-everything-works-together)
5. [Example Walkthrough](#example-walkthrough)
6. [Common Interview Questions](#common-interview-questions)
7. [Troubleshooting](#troubleshooting)

---

## Project Overview

### What This Project Does
This project implements the **first two phases of a compiler**:
1. **Lexical Analysis (Scanning)** - Converts source code into tokens
2. **Syntax Analysis (Parsing)** - Validates grammar and builds parse tree

### Tools Used
- **Flex** (Fast Lexical Analyzer Generator) - Generates the lexer from `lex_analyzer.l`
- **Bison/Yacc** (Yet Another Compiler Compiler) - Generates the parser from `syntax_analyzer.y`
- **G++** - Compiles the generated C++ code

### Programming Language Being Parsed
A **C-like language** with features like:
- Data types: `int`, `float`, `char`, `double`, `void`
- Control structures: `if`, `else`, `for`, `while`, `return`
- Operators: `+`, `-`, `*`, `/`, `%`, `&&`, `||`, `!`, `==`, `!=`, etc.
- Functions with parameters
- Arrays

---

## Compilation Process

### Step-by-Step Breakdown

#### Command Flow in `script.sh`:

```bash
#!/bin/bash

# Step 1: Generate parser from Bison/Yacc file
yacc -d -y syntax_analyzer.y
# Output: y.tab.c (parser code) and y.tab.h (token definitions)

# Step 2: Compile parser to object file
g++ -c y.tab.c -o y.o

# Step 3: Generate lexer from Flex file
flex lex_analyzer.l
# Output: lex.yy.c (lexer code)

# Step 4: Compile lexer to object file
g++ -c lex.yy.c -o l.o

# Step 5: Link both object files into executable
g++ y.o l.o -o parser

# Step 6: Run the parser with input file
./parser input.txt
```

### What Each File Does

| File | Purpose | Generated By |
|------|---------|--------------|
| `syntax_analyzer.y` | Grammar rules definition | You (manually) |
| `lex_analyzer.l` | Token patterns definition | You (manually) |
| `symbol_info.h` | Token data structure | You (manually) |
| `y.tab.c` | Parser implementation | Bison/Yacc |
| `y.tab.h` | Token definitions (#define) | Bison/Yacc |
| `lex.yy.c` | Lexer implementation | Flex |
| `y.o` | Compiled parser | G++ |
| `l.o` | Compiled lexer | G++ |
| `parser` | Final executable | G++ (linker) |

---

## Component Breakdown

### 1. Symbol Info (`symbol_info.h`)

```cpp
class symbol_info {
private:
    string sym_name;  // The actual text (lexeme)
    string sym_type;  // Type of token (ID, CONST_INT, etc.)

public:
    symbol_info(string name, string type) {
        sym_name = name;
        sym_type = type;
    }
    
    string getname() { return sym_name; }
    string gettype() { return sym_type; }
};
```

**Purpose**: Stores information about each token
- **sym_name**: The actual text from source code (e.g., "main", "123", "if")
- **sym_type**: Category of token (e.g., "ID", "CONST_INT", "KEYWORD")

**Example**:
```cpp
symbol_info s1("main", "ID");          // Identifier
symbol_info s2("123", "CONST_INT");    // Integer constant
symbol_info s3("+", "ADDOP");          // Addition operator
```

---

### 2. Lexical Analyzer (`lex_analyzer.l`)

#### Structure of Flex File

```lex
%option noyywrap

%{
    /* C/C++ declarations and includes */
    #include<bits/stdc++.h>
    #include"symbol_info.h"
    #include "y.tab.h"  // Token definitions from Bison
    
    extern YYSTYPE yylval;  // Value passed to parser
    extern int lines;        // Line counter
    extern ofstream outlog;  // Log file
%}

/* Regular Expression Definitions */
delim     [ \t\v\r]
ws        {delim}+
newline   \n
letter_   [A-Za-z_]
digit     [0-9]
id        {letter_}({letter_}|{digit})*
integer   {digit}+
float     {digit}+\.{digit}+

%%
/* Pattern Matching Rules */

{ws}        { /* Ignore whitespace */ }
{newline}   { lines++; }

{integer}   {
                symbol_info *s = new symbol_info((string)yytext,"CONST_INT");
                yylval = (YYSTYPE)s;
                return CONST_INT;
            }

{id}        {
                symbol_info *s = new symbol_info((string)yytext,"ID");
                yylval = (YYSTYPE)s;
                return ID;
            }

"if"        { return IF; }
"while"     { return WHILE; }
"+"         { return ADDOP; }
"("         { return LPAREN; }
")"         { return RPAREN; }

%%
/* User code section */
```

#### How Lexical Analysis Works

**Process**:
1. Read input character by character
2. Match longest possible pattern
3. Execute corresponding action
4. Return token type to parser

**Example Input**: `int x = 5;`

**Lexer Output**:
```
Token: INT,        Lexeme: "int"
Token: ID,         Lexeme: "x"
Token: ASSIGNOP,   Lexeme: "="
Token: CONST_INT,  Lexeme: "5"
Token: SEMICOLON,  Lexeme: ";"
```

#### Key Variables

- **`yytext`**: Contains the matched text (lexeme)
- **`yylval`**: Value passed to parser (stores symbol_info)
- **`lines`**: Current line number
- **`outlog`**: Output log file stream

#### Regular Expression Examples

```lex
digit     [0-9]               // Matches: 0,1,2,...,9
letter_   [A-Za-z_]           // Matches: a-z, A-Z, _
id        {letter_}({letter_}|{digit})*  // Matches: x, var1, _temp, count
integer   {digit}+            // Matches: 123, 5, 9999
float     {digit}+\.{digit}+  // Matches: 3.14, 0.5, 123.456
```

---

### 3. Syntax Analyzer (`syntax_analyzer.y`)

#### Structure of Bison File

```yacc
%{
    /* C/C++ declarations */
    #include<bits/stdc++.h>
    #include"symbol_info.h"
    
    int yyparse(void);
    int yylex(void);
    extern FILE *yyin;
    
    ofstream outlog;
    int lines = 1;
    
    void yyerror(char *s) {
        cout << "Error: " << s << " at line " << lines << endl;
    }
%}

/* Token declarations */
%token IF ELSE FOR WHILE
%token INT FLOAT CHAR DOUBLE VOID
%token ID CONST_INT CONST_FLOAT
%token ADDOP MULOP RELOP ASSIGNOP
%token LPAREN RPAREN LCURL RCURL SEMICOLON

/* Operator precedence (lowest to highest) */
%right ASSIGNOP
%left LOGICOP
%left RELOP
%left ADDOP
%left MULOP

%%
/* Grammar Rules */

start : program
    {
        outlog << "start : program" << endl;
    }
    ;

program : program unit
    {
        $$ = new symbol_info($1->getname()+"\n"+$2->getname(),"program");
    }
    | unit
    {
        $$ = $1;
    }
    ;

unit : var_declaration
    | func_definition
    ;

var_declaration : type_specifier declaration_list SEMICOLON
    {
        $$ = new symbol_info($1->getname()+" "+$2->getname()+";","var_declaration");
    }
    ;

expression : logic_expression
    | variable ASSIGNOP logic_expression
    ;

%%

/* Main function */
int main(int argc, char *argv[])
{
    yyin = fopen(argv[1], "r");
    outlog.open("my_log.txt", ios::trunc);
    
    yyparse();
    
    outlog.close();
    fclose(yyin);
    return 0;
}
```

#### Grammar Rule Format

```yacc
nonterminal : production_1
            | production_2
            | production_3
            {
                /* Action code (C++) */
                $$ = result;        // Left-hand side
                $1, $2, $3...       // Right-hand side symbols
            }
            ;
```

#### Special Variables in Actions

- **`$$`**: Value of the **left-hand side** (result of this rule)
- **`$1, $2, $3...`**: Values of **right-hand side** symbols (in order)

**Example**:
```yacc
expression : term ADDOP term
    {
        // $1 = first term
        // $2 = ADDOP (+)
        // $3 = second term
        $$ = new symbol_info($1->getname()+$2->getname()+$3->getname(),"expression");
    }
    ;
```

Input: `x + y`
- `$1` = symbol_info for "x"
- `$2` = symbol_info for "+"
- `$3` = symbol_info for "y"
- `$$` = symbol_info for "x+y"

---

## How Everything Works Together

### The Complete Flow

```
┌─────────────┐
│ input.txt   │
│ Source Code │
└──────┬──────┘
       │
       ▼
┌─────────────────┐
│  Lexer (Flex)   │  ← lex_analyzer.l
│  lex.yy.c       │
└──────┬──────────┘
       │ Tokens
       ▼
┌─────────────────┐
│ Parser (Bison)  │  ← syntax_analyzer.y
│  y.tab.c        │
└──────┬──────────┘
       │ Parse Tree
       ▼
┌─────────────────┐
│  my_log.txt     │
│  Parse Log      │
└─────────────────┘
```

### Detailed Execution Flow

#### 1. **Parser Initialization** (`main()` in .y file)
```cpp
yyin = fopen("input.txt", "r");    // Open input file
outlog.open("my_log.txt");         // Open log file
yyparse();                          // Start parsing
```

#### 2. **Parser Requests Token**
```cpp
// Inside y.tab.c (generated)
token = yylex();  // Call lexer to get next token
```

#### 3. **Lexer Scans Input**
```cpp
// Inside lex.yy.c (generated)
// Read characters from yyin
// Match pattern
// Return token type (e.g., IF, ID, CONST_INT)
```

#### 4. **Parser Applies Grammar Rules**
```cpp
// Match tokens against grammar rules
// Build parse tree
// Execute action code
// Log to my_log.txt
```

#### 5. **Repeat Until End of File**

---

## Example Walkthrough

### Input File (`input.txt`)

```c
int main()
{
    int x;
    x = 5;
    return x;
}
```

### Step-by-Step Token Generation

| Step | Input Position | Token Type | Lexeme | Action |
|------|---------------|------------|--------|---------|
| 1 | `int` | INT | "int" | Return INT token |
| 2 | `main` | ID | "main" | Create symbol_info("main","ID") |
| 3 | `(` | LPAREN | "(" | Return LPAREN token |
| 4 | `)` | RPAREN | ")" | Return RPAREN token |
| 5 | `{` | LCURL | "{" | Return LCURL token |
| 6 | `int` | INT | "int" | Return INT token |
| 7 | `x` | ID | "x" | Create symbol_info("x","ID") |
| 8 | `;` | SEMICOLON | ";" | Return SEMICOLON token |
| 9 | `x` | ID | "x" | Create symbol_info("x","ID") |
| 10 | `=` | ASSIGNOP | "=" | Return ASSIGNOP token |
| 11 | `5` | CONST_INT | "5" | Create symbol_info("5","CONST_INT") |
| 12 | `;` | SEMICOLON | ";" | Return SEMICOLON token |
| 13 | `return` | RETURN | "return" | Return RETURN token |
| 14 | `x` | ID | "x" | Create symbol_info("x","ID") |
| 15 | `;` | SEMICOLON | ";" | Return SEMICOLON token |
| 16 | `}` | RCURL | "}" | Return RCURL token |

### Parse Tree Construction

```
start
  └─ program
       └─ unit
            └─ func_definition
                 ├─ type_specifier (int)
                 ├─ ID (main)
                 ├─ LPAREN
                 ├─ RPAREN
                 └─ compound_statement
                      ├─ LCURL
                      ├─ statements
                      │    ├─ statement (var_declaration)
                      │    │    └─ int x;
                      │    ├─ statement (expression_statement)
                      │    │    └─ x = 5;
                      │    └─ statement (return_statement)
                      │         └─ return x;
                      └─ RCURL
```

### Generated Log (`my_log.txt`)

```
Line no 1: Token <INT> Lexeme int found

Line no 1: Token <ID> Lexeme main found

At line no: 1 type_specifier : INT

At line no: 1 func_definition : type_specifier ID LPAREN RPAREN compound_statement

At line no: 6 program : unit

At line no: 6 start : program

Total lines: 6
```

---

## Common Interview Questions

### 1. **What is the difference between lexical and syntax analysis?**

**Answer**: 
- **Lexical Analysis**: Breaks source code into tokens (words). It's like splitting a sentence into words.
  - Input: `int x = 5;`
  - Output: `[INT] [ID:x] [ASSIGNOP:=] [CONST_INT:5] [SEMICOLON]`

- **Syntax Analysis**: Checks if tokens follow grammar rules. It's like checking if a sentence is grammatically correct.
  - Input: `[INT] [ID:x] [ASSIGNOP:=] [CONST_INT:5] [SEMICOLON]`
  - Output: Valid variable declaration

### 2. **Why use Flex and Bison instead of writing the lexer/parser manually?**

**Answer**:
- **Automatic generation**: Saves time and reduces bugs
- **Optimized code**: Generated code is highly optimized
- **Easy to modify**: Change patterns/grammar without rewriting entire lexer/parser
- **Industry standard**: Used in real compilers (GCC uses them)

### 3. **What is a token?**

**Answer**: A token is a **categorized unit of the source code**. It has two parts:
- **Token Type**: Category (ID, KEYWORD, OPERATOR, etc.)
- **Lexeme**: Actual text from source

Example:
```c
int count = 42;
```
Tokens:
- `<INT, "int">`
- `<ID, "count">`
- `<ASSIGNOP, "=">`
- `<CONST_INT, "42">`
- `<SEMICOLON, ";">`

### 4. **What happens if there's a syntax error?**

**Answer**: The parser calls `yyerror()` function:
```cpp
void yyerror(char *s) {
    cout << "Error at line " << lines << ": " << s << endl;
}
```

Example:
```c
int x = ;  // Missing value
```
Output: `Error at line 1: syntax error`

### 5. **Explain operator precedence in your parser**

**Answer**: Defined in Bison file using `%left`, `%right`:
```yacc
%right ASSIGNOP    // Right associative (a = b = c)
%left LOGICOP      // &&, ||
%left RELOP        // <, >, ==, !=
%left ADDOP        // +, -
%left MULOP        // *, /, %
%right NOT         // !
```

Lower in the list = **higher precedence**

Example: `x = 2 + 3 * 4`
- Multiplication happens first (higher precedence)
- Result: `x = 2 + 12` → `x = 14`

### 6. **What is the purpose of `yylval`?**

**Answer**: `yylval` is used to **pass semantic value from lexer to parser**.

```cpp
// In lexer (lex_analyzer.l)
{id} {
    symbol_info *s = new symbol_info(yytext, "ID");
    yylval = s;  // Pass to parser
    return ID;   // Token type
}

// In parser (syntax_analyzer.y)
// $1, $2, $3 access yylval values
```

### 7. **What does `#define YYSTYPE symbol_info*` do?**

**Answer**: It tells Bison/Flex that the **data type of all tokens** is `symbol_info*` (pointer).

Without it, `yylval` would be just an `int`.

### 8. **Why are there warnings about "useless rules"?**

**Answer**: Some grammar rules are **defined but never reachable** from the start symbol.

Example:
```yacc
start : program ;
program : unit ;
unit : func_definition ;
func_definition : type_specifier ID LPAREN RPAREN compound_statement ;
compound_statement : LCURL RCURL ;  // Only this rule is used

// This rule exists but is NEVER reached:
compound_statement : LCURL statements RCURL ;  // ← Useless!
```

**Fix**: Connect the rules properly so they can be reached.

### 9. **What is the parse tree?**

**Answer**: A **hierarchical representation** of how the input matches the grammar.

Input: `x = 5;`

Parse Tree:
```
expression_statement
  └─ expression
       ├─ variable (x)
       ├─ ASSIGNOP (=)
       └─ logic_expression
            └─ rel_expression
                 └─ simple_expression
                      └─ term
                           └─ unary_expression
                                └─ factor
                                     └─ CONST_INT (5)
```

### 10. **Explain the compilation steps**

**Answer**:
```
Source Code (.l, .y)
    ↓
Code Generation (Flex, Bison)
    ↓
Generated C Code (.c)
    ↓
Compilation (g++)
    ↓
Object Files (.o)
    ↓
Linking (g++)
    ↓
Executable (parser)
```

---

## Troubleshooting

### Common Errors and Solutions

#### 1. **"cannot open: No such file or directory"**
```
/usr/bin/bison: syntax_analyzer.y: cannot open
```
**Solution**: Check filename in script.sh matches actual file name.

#### 2. **"symbol 'XXX' is used, but is not defined as a token"**
```
error: symbol 'INT' is used, but is not defined as a token
```
**Solution**: Add token declaration in .y file:
```yacc
%token INT FLOAT CHAR
```

#### 3. **"fatal error: y.tab.h: No such file or directory"**
```
lex_analyzer.l:10:10: fatal error: y.tab.h
```
**Solution**: Bison failed. Check .y file for errors. Fix them first.

#### 4. **"warning, rule cannot be matched"**
```
lex_analyzer.l:56: warning, rule cannot be matched
```
**Solution**: Regular expression is incomplete:
```lex
ws        {delim}+      // Correct
ws                      // Wrong - incomplete!
```

#### 5. **"undefined reference to yylex"**
```
undefined reference to `yylex()'
```
**Solution**: Link both lexer and parser object files:
```bash
g++ y.o l.o -o parser
```

#### 6. **"nonterminal useless in grammar"**
```
warning: nonterminal useless in grammar: statements
```
**Solution**: Connect the rule to the main grammar tree so it's reachable from `start`.

---

## Key Takeaways for Interview

### What You Built
✅ A **lexical analyzer** that tokenizes C-like code  
✅ A **syntax analyzer** that validates grammar  
✅ A **parser** that builds parse trees  
✅ Integration using **industry-standard tools** (Flex/Bison)

### Skills Demonstrated
✅ Understanding of **compiler theory**  
✅ **Regular expressions** for pattern matching  
✅ **Context-free grammars** for syntax rules  
✅ **C++ programming** and file I/O  
✅ **Build automation** with shell scripts  
✅ **Debugging** compilation errors

### Real-World Applications
- Compilers (GCC, Clang, Java compiler)
- Interpreters (Python, JavaScript)
- Code formatters (Prettier, clang-format)
- Static analyzers (ESLint, SonarQube)
- IDEs (syntax highlighting, error detection)

---

## Practice Questions

Before your interview, make sure you can answer:

1. Trace the token generation for: `if (x > 5) { return x; }`
2. Draw the parse tree for: `int x = 2 + 3 * 4;`
3. Explain what happens when the lexer encounters `123abc` (is it valid?)
4. How would you add support for comments (`//` and `/* */`)?
5. What's the difference between `%left` and `%right` for operators?
6. How does the parser handle ambiguous grammars?
7. What would you do to add support for `printf("Hello");`?

---

## Final Tips

✅ **Run your code** before the interview - know what works and what doesn't  
✅ **Understand the flow** - from source to tokens to parse tree  
✅ **Be honest** about incomplete parts  
✅ **Focus on concepts** - they care more about understanding than perfection  
✅ **Prepare to explain** - walk through an example on a whiteboard

**Good luck with your interview! 🚀**
